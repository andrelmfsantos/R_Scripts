---
title: "TITLE"
author:
- "AUTOR 1"
- "AUTOR 2"
- "AUTOR 3"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = c('pdf', 'png'),
                      dpi=200
                      )
library(readr)
library(nFactors)
library(psych)
library(ggplot2)
library(igraph)
library(flextable)
library(FactorAssumptions)
library(ggraph)
library(tidygraph)
options(scipen = 999)
```

**Observação**: Caso as tabelas explodam na página do Word, ir em `table > AutoFit to Window` ou `Tabela > AutoAjuste de Janela`.

## Coleta de Dados

Buscamos no Scopus a seguinte expressão de busca: `SEARCH_EXPRESSION`

Total coletados pela expressão de busca: 153

## Top-cited

Abaixo os `15` documentos mais citados pela amotra de `XXX` artigos

```{r top-cited, message=F, warning=F}
# Load the data
topcited <- read_tsv("co-cit-pareamento.cit", col_names = c("citations", "document"), col_types = "ic")
topcited_table <- head(topcited, 15)
topcited_table <- flextable(as.data.frame(topcited_table))
topcited_table <- autofit(topcited_table)
topcited_table
```
## BibExcel

A matriz de co-citação foi gerada no `BibExcel`. O total de citações pela amostra é `XXX` citações. Selecionamos o `top-XX` citados (mínimo de `X` citações) totalizando em `XXX` citações (`XX%` das citações totais).

```{r data, message=F, warning=F}
# Load the data
data <- read_tsv("co-cit-1988-2010.ma2")
data <- data[,1:(length(data)-1)] #removing the last column
data <- as.data.frame(data)
rownames(data) <- colnames(data)

# If you want to do a coupling
#cols_coupling <- c()
#for (name in names(data)) {
#  cols_coupling_individual <- paste0("bc", name)
#  cols_coupling <- c(cols_coupling, cols_coupling_individual)
#}
#colnames(data) <- cols_coupling
#rownames(data) <- colnames(data)
```

## Remoção de KMO

Dos `XX` artigos `XX` tiveram quer se removidos pois não possuem KMO individual maior que 0.5 (critério mínimo de aceitação para uma análise fatorial ou de componentes).

Foram removidos de maneira iterativa. Ou seja, removendo a variável com menor KMO e repetindo o procedimento com as restantes até obter uma solução final em que todos os KMOs invididuais estejam com valores acima de 0.5. Abaixo estão listados os removidos ordenados por ordem de remoção.

Amostra após remoção de KMO = `XX` documentos

```{r KMO, echo=FALSE, warning=F, message=F}
#  Removing KMO
factor_analysis <- kmo_optimal_solution(data, squared = T)
sink("removed_kmo.txt")
factor_analysis$removed
sink()
data <- factor_analysis$df

# Rendering as FlexTable
kmo_removed <- flextable(as.data.frame(factor_analysis$removed))
kmo_removed <- autofit(kmo_removed)
kmo_removed

# If error try this
#print(factor_analysis$removed)
```

## Desvio Padrão Nulo

Também é necessário checar se há variáveis com desvio padrão nulo ($sd = 0$).

Abaixo estão listadas as variáveis que foram removidas.

Total= `XX` artigos

```{r sd-NULL, echo=F, warning=F, message=F}
#removing zero std_dev variables
sd_zero <- names(data[,sapply(data, function(x) { sd(x) == 0} )])
sink("removed_sd_zero.txt")
sd_zero
sink()
data <- data[!sapply(data, function(x) { sd(x) == 0} ), !sapply(data, function(x) { sd(x) == 0} ), drop = FALSE]
# Rendering as FlexTable
sd_zero <- flextable(as.data.frame(sd_zero))
sd_zero <- autofit(sd_zero)
sd_zero
```

## Bartlett-Sphericity test e KMO Geral

Se der `NaN` (*Not a Number*) é um erro que geralmente indica que o p-valor do barlett é muito pequeno.

```{r barlett, echo=F, warning=F}
bartlett <- cortest.bartlett(cor(data), n = length(data))$p.value
kmo_geral <- factor_analysis$results$overall
cat("Bartlett:", bartlett, "\n")
cat("KMO Geral:", kmo_geral, "\n")
```

## Quantos Fatores?

Essa pergunta é perene em análises fatoriais/componentes. 

You can use the **Parallel Analysis**. The logic underlying this approach is that the magnitude of the eigenvalue for the last retained factor should exceed an eigenvalue obtained from random data under otherwise comparable conditions. In other words, in a real study involving the factor analysis of, say, 20 variables measured on 500 people, the eigenvalue of any retained factor should be greater than the corresponding eigenvalue obtained from randomly generated data arranged to represent 20 variables measured on 500 people. We generally use a representative value (e.g., the 95% percentile).

Abaixo estão listadas as soluções ideias para o número de fatores/componentes conforme:

  * **Kaiser Rule**: Eigenvalues maiores que 1
  $n_{Kaiser} = ∑_{i} (λ_{i} ≥ \bar{λ}).$
  Nota que se usar uma matriz de correlação (nosso caso) $\bar{λ}$ é 1.
  
  * **Parallel Analysis**: Usando o percentil 95%
  $n_{parallel} = ∑_{i} (λ_{i} ≥ LS_i).$
  
  * **Acceleration factor**: Indica o *cotovelo* do Scree Plot. Corresponde à aceleração da curva (segunda derivada)
  $n_{AF} \equiv \ If \ ≤ft[ (λ_{i} ≥ LS_i) \ and \ max(AF_i) \right]$
  
  * **Optimal Coordinates**: As *coordenadas otimizadas* são extrapoladas de um valor prévio de Eigenvalue que permite que o eigenvalues observado tenha um valor além dessa extrapilação. A extrapolação é feita por uma regressão linear usando a coordenada do último eigenvalue e a coordenada do eigenalue $k+1$. No total há $k-2$ linhas de regressão como essas.
  $n_{OC} = ∑_i ≤ft[(λ_i ≥ LS_i) \cap (λ_i ≥ (λ_{i \ predicted}) \right].$

```{r how-many-factors, echo=F, warnings=F, message=F}
ev <- eigen(cor(data)) # get eigenvalues
ap <- nFactors::parallel(subject = nrow(data),var = ncol(data),rep = 1000, quantile = 0.05)
nS <- nFactors::nScree(x = ev$values, aparallel = ap$eigen$qevpea)
plotnScree(nS)
```

## Análise Fatorial/Componentes

Como a análise paralela sugeriu `XX` fator. O número de fatores a serem extraídos pela análise fatorial/componentes é `XX`.

```{r factor analysis, warning=F}
results <- principal(cor(data), nfactors = 1, scores = T, rotate = "varimax")
```

### Variância Explicada

```{r variance_explained, echo=F, message=F, warning=F}
write.csv2(results$Vaccounted, "variance_accounted.csv")
Vaccounted <- cbind("stats" = rownames(results$Vaccounted), data.frame(results$Vaccounted, row.names=NULL))
Vaccounted <- flextable(Vaccounted)
Vaccounted <- autofit(Vaccounted)
Vaccounted
```

## Cargas Fatoriais

```{r loadings, echo=F,warning=F, message=F}
write.csv2(results$loadings, "loadings_bruto.csv")
# Export Factor ID for every variable
# It is important to use absolute values because of negative loadings
df <- data.frame(unclass(results$loadings), stringsAsFactors = F)
factors <- data.frame(
  "ID" = paste0("cc", 1:nrow(data)),
  "name" = rownames(df),
  "factor" = apply(df[1:ncol(df)], 1, function(x) colnames(df)[which.max(abs(x))]),
  stringsAsFactors = F)
loadings_table <- data.frame(
  "ID" = paste0("cc", 1:nrow(data)),
  "name" = rownames(df)
)
loadings_table <- cbind(loadings_table,df)
loadings_table_pretty <- flextable(loadings_table)
loadings_table_pretty <- autofit(loadings_table_pretty)
loadings_table_pretty
```

## Social Networks

Here are some stats of the Network:

```{r adjacency_matrix, include=F}
# Saving Data
write.csv(data, "adjacency_matrix.csv")
```
```{r igraph_data, echo=F, warning=F, message=F}
# Creating network using igraph
data <- read.csv("adjacency_matrix.csv", row.names = 1) # if you need to load network
g <- graph_from_adjacency_matrix(as.matrix(data), mode = "undirected", weighted = T, diag = F)

# Vertex Atributes
V(g)$factor <- factors$factor
V(g)$ID <- factors$ID
V(g)$degree <- degree(g, normalized = F)
V(g)$Ndegree <- degree(g, normalized = T)
V(g)$betweenness <- betweenness(g)
V(g)$Nbetweenness <- betweenness(g, normalized = T)

# Centralization
network_centrality <- centr_eigen(g, directed = F)$centralization #closeness of vertices
# Cohesion
network_cohesion <- cohesion(g)
# Density
# Make sure to always simplify an undirected graph to remove loops and multiple edges 
# before calculating its density
# network.density(asNetwork(igraph::simplify(g)))
network_density <- graph.density(igraph::simplify(g),loop=FALSE)

cat("Network Centrality:", network_centrality, "\n")
cat("Network Cohesion:", network_cohesion, "\n")
cat("Network Density:", network_density, "\n")
```

### Plots

**Observações**:

  * Tamanho do Node é `NDegree`
  * Cor do Node é fator correspondente da maior carga *absoluta*
  * Largura da Edge é o `weight` (*frequência da co-ocorrência*)
  * Layout é `Fruchterman-Reingold`


```{r plot_network_fr, warning=F, message=F}
ggraph(g, layout = 'fr') + 
  geom_edge_link() + 
  geom_node_point(aes(size = centrality_degree(normalized = T), colour = factor)) +
  geom_node_text(aes(label = ID, size = 2), repel = T, size = 1)
```

### Sub-networks

Abaixo estão alguns atributos para cada fator, tomando como base que cada um é uma rede.

```{r sub-network, echo=F, warning=F, message=F}
sub_df <- data.frame()

for (i in unique(na.omit(V(g)$factor))){
  #variance_accounted <- which(results$Vaccounted)
  new_g <- induced.subgraph(g, which(V(g)$factor==as.character(i)))
  centrality_degree <- centr_degree(new_g)$centralization
  centrality_eigen <- centr_eigen(new_g, directed = F)$centralization
  sub_cohesion <- mean(degree(new_g))/(vcount(new_g)-1)
  sub_density <- graph.density(igraph::simplify(new_g),loop=FALSE)
  dat <- data.frame("factor" = as.factor(i),
                    "centrality_degree" = centrality_degree,
                    "centrality_eigen" = centrality_eigen,
                    "cohesion" = sub_cohesion,
                    "density" = sub_density)
  sub_df <- rbind(sub_df, dat)
}
variance_transposed <- as.data.frame(t(results$Vaccounted), stringsAsFactors = F)
variance_transposed$factor <- rownames(variance_transposed)
sub_df$variance_accounted <- variance_transposed[match(sub_df$factor, variance_transposed$factor), 2]
sub_df$kmo_geral <- kmo_geral
sub_df$bartlett_geral <- bartlett
sub_networks_table <- flextable(sub_df)
sub_networks_table <- autofit(sub_networks_table)
sub_networks_table
```

## Fatorial com Atributos de Nodes

Abaixo a tabela de fatorial com atributos de Nodes:
  * **degree**: Degree absoluto
  * **Ndegree**: Degree normalizado (entre 0 e 1)
  * **betweenness**: Betweenness absoluto
  * **Nbetweenness**: Betweenness normalizado (entre 0 e 1)
  
```{r loadings_degree, echo=F, warning=F, message=F}
loadings_table_degree <- data.frame(
  "degree" = V(g)$degree, 
  "Ndegree" = V(g)$Ndegree,
  "betweenness" = V(g)$betweenness,
  "Nbetweenness" = V(g)$Nbetweenness
  )
loadings_table_degree <- cbind(loadings_table, loadings_table_degree)
loadings_table_degree_pretty <- flextable(loadings_table_degree)
loadings_table_degree_pretty <- autofit(loadings_table_degree_pretty)
loadings_table_degree_pretty

```

## Export to Gephi

* Node List com fatores
* Adjacency Matrix com as co-ocorrências
* DOT file (This is the best for Gephi)

```{r}
# Adjacency Matrix
write.csv(data, "adjacency_matrix.csv")

# Node list
node_list <- data.frame(Node = V(g)$name, Factor = V(g)$factor)
write.csv(node_list, "node_list_factor.csv")

# Network.dot
write_graph(g, "network.dot", format = "dot")
```

## Session Info
Para replicação

```{r}
sessionInfo()
```

