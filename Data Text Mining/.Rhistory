prod <- str_remove_all(prod, "^[t\t1.\\s]$")
str_match(prod, "^[t\t1.\\s]$")
str_match(prod, "^[t1.]\\s$")
str_match(prod, "[t1.]\\s$")
str_match(prod, "*[t1.]\\s$")
str_match(prod, "[t1.]\\s$")
prod <- str_remove_all(prod, "[t1.]\\s$")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
str_match(prod, "^[Artigos]*\\s$")
str_match(prod, "^[Artigos]\\w*\\s$")
str_match(prod, "^[Artigos]\\*\\s$")
str_match(prod, "^[Artigos]")
str_match(prod, "^[A*]")
str_match(prod, "^[A*]\\*\\s$")
str_match(prod, "^[A*]*\\s$")
str_match(prod, "^[A*]\\s$")
str_match(prod, "^[A*]$\\s")
str_match(prod, "^\\w*$\\s")
str_match(prod, "^\\w*\\s")
str_match(prod, "^\\w*\\s$")
str_match(prod, regex("^\\w*\\s$"))
str_match(prod, regex("^[t1.]\\s$"))
str_match(prod, regex("^[t1.]*\\s$"))
prod
prod <- str_remove(prod, "^(Artigos.+).+t1\\.\\s")
prod <- str_remove(prod, "^(Artigos.+).+t1\\.\\s")
prod <- gsub("[t1.]\\s$","", prod)
prod <- gsub("[t1.]\\s$","", prod)
prod <- gsub("^(Artigos.+).+t1\\.\\s","", prod)
prod <- gsub("^(Artigos.+).+t1\\.\\s","", prod)
prod <- gsub("^\\(Artigos.+).+t1\\.\\s","", prod)
prod <- gsub("^\\(Artigos.+)+t1\\.\\s","", prod)
prod <- gsub("^\\(Artigos.+)t1\\.\\s","", prod)
prod <- str_remove(prod, "^([Artigos.+].+t1\\.\\s")
prod <- str_remove(prod, "^[Artigos.+].+t1\\.\\s")
prod <- str_remove(prod, "^[Artigos.+].+[t1]\\.\\s")
prod
prod <- str_remove(prod, "^[Artigos.+].+[t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- gsub("^[Artigos.+].+[t1]\\.\\s","", prod)
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^[Artigos.+].+[t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod
prod <- str_remove(prod, "^[Artigos.+].+[\t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- gsub("^[Artigos.+].[t1]\\.\\s","", prod)
prod <- gsub("^[Artigos]+.+[t1]\\.\\s","", prod)
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos.+]).+[\t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "(^([Artigos.+]).+[\t1]\\.\\s)")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos].+).+\\[t1]\.\s")
prod <- str_remove(prod, "^([Artigos].+).+\\\[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos].+).+\\[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos].+).+[t1]\\.\\s")
l
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos]).[t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos]).[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos]).+[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos]).*[t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos]).*[t1]\\.\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "^([Artigos]).*\\[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos]).*\\[t1]\\.\\s")
prod <- str_remove(prod, "^([Artigos]).*t1\\.\\s")
prod <- str_remove(prod, "^([Artigos]).*[t1.]\\s")
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- str_remove(prod, "(^[Artigos]).*[t1.]\\s")
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
prod <- gsub("(^[Artigos]).*[t1.]\\s","", prod)
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
#spliting as generting a df
prod_data <- as.data.frame(str_split(prod, "\\s\\d{1,2}\\.\\s"))
prod_data <- as.data.frame(prod_data[-1,])
View(prod_data)
prod_data <- data.frame(lapply(prod_data, function(x){
gsub(".\\.\\d{4}", "", x)
}))
#spliting as generting a df
prod_data <- as.data.frame(str_split(prod, "\\s\\d{1,2}\\.\\s"))
prod_data <- as.data.frame(prod_data[-1,])
for (i in 1:nrow(prod_data)) {
i <- str_remove(i, ".\\.\\d{4}")
}
View(prod_data)
prod_data <- data.frame(lapply(prod_data, function(x){
gsub(".\\.\\d{4}", "", x)
}))
View(prod_data)
#spliting as generting a df
prod_data <- as.data.frame(str_split(prod, "\\s\\d{1,2}\\.\\s"))
prod_data <- as.data.frame(prod_data[-1,])
prod_data <- data.frame(lapply(prod_data, function(x){
gsub(".*\\.\\d{4}", "", x)
}))
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("[A-Z]*\\,\\w*.\\d{4}", "", x)
}))
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("[A-Z]*\\,\\s\\w*.\\d{4}", "", x)
}))
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("[A-Z]*\\,\\s\\w*\\s\\w*\\d{4}", "", x)
}))
gsub("\\[A-Z]*\\,\\s\\w*\\s\\w*\\d{4}", "", x)
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("\\[A-Z]*\\,\\s\\w*\\s\\w*\\d{4}", "", x)
}))
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("\\w*\\,\\s\\w*\\s\\w*\\d{4}", "", x)
}))
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
#removing first line duplicates
prod_data <- list(lapply(prod, function(x){
gsub(".*\\.\\d{4}", "", x)
}))
View(prod_data)
prod_data[[1]][[1]]
#removing first line duplicates
prod_data <- data.frame(lapply(prod, function(x){
gsub(".*\\.\\d{4}", "", x)
}))
View(prod_data)
prod_data
#only complete articles
prod_html <- html_nodes(webpage, '#artigos-completos')
prod <- html_text(prod_html)
#remove first duplicate author
prod <- gsub("\n","",prod)
#spliting as generting a df
prod_data <- as.data.frame(str_split(prod, "\\s\\d{1,2}\\.\\s"))
prod_data <- as.data.frame(prod_data[-1,])
#removing first line duplicates
prod_data <- data.frame(lapply(prod_data, function(x){
gsub(".*\\.\\d{4}", "", x)
}))
prod_data <- data.frame(lapply(prod_data, function(x){
gsub("\\w*\\,\\s\\w*\\s\\w*\\d{4}", "", x)
}))
View(prod_data)
prod_data
#insert LATTES URL
url <- 'http://lattes.cnpq.br/2281909649311607'
webpage <- read_html(url)
#only complete articles
prod_html <- html_nodes(webpage, '#artigos-completos')
prod <- html_text(prod_html)
prod <- html_text(prod_html)
library(dplyr)
library(stringr)
library(rvest)
#insert LATTES URL
url <- 'http://lattes.cnpq.br/2281909649311607'
webpage <- read_html(url)
#only complete articles
prod_html <- html_nodes(webpage, '#artigos-completos')
prod <- html_text(prod_html)
prod
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(tidyverse)
library(tidytext)
library(rvest)
library(xlsx)
library(ggplot2)
library(igraph)
library(ggraph)
library(widyr)
library(xlsx)
library(tm)
install.packages('tidytext')
install.packages('xlsx')
install.packages('widyr')
install.packages('tm')
#Pegar o bruto savedrecs do WoS formatado no Excel
savedrecs <- read.delim('lamano_text.txt', header = F, sep = "\t")
#Pegar o bruto savedrecs do WoS formatado no Excel
savedrecs <- read.delim('lamano_text.txt', header = F, sep = "\t", colClasses = 'chr')
#Pegar o bruto savedrecs do WoS formatado no Excel
savedrecs <- read.delim('lamano_text.txt', header = F, sep = "\t", colClasses = 'char')
#Pegar o bruto savedrecs do WoS formatado no Excel
savedrecs <- read.delim('lamano_text.txt', header = F, sep = "\t",
colClasses = 'character')
head(savedrecs)
#separar as palavras dos rotulo de dados
bruto <- data_frame(txt = savedrecs) #usar rotulos do WoS
#separar as palavras dos rotulo de dados
bruto <- data_frame(savedrecs) #usar rotulos do WoS
#Pegar o bruto savedrecs do WoS formatado no Excel
savedrecs <- read.delim('lamano_text.txt', header = F,
colClasses = 'character')
#separar as palavras dos rotulo de dados
bruto <- data_frame(savedrecs$V1) #usar rotulos do WoS
#Pegar o bruto savedrecs do WoS formatado no Excel
bruto <- read.delim('lamano_text.txt', header = F,
colClasses = 'character')
head(bruto)
bruto_word <- bruto %>%
unnest_tokens(word, txt)
#remover stop words
data(stop_words)
library(widyr)
library(tm)
library(xlsx)
library(tidyverse)
library(tidytext)
bruto_word <- bruto_word %>%
anti_join(stop_words)
#separar as palavras dos rotulo de dados
bruto <- data_frame(savedrecs$V1) #usar rotulos do WoS
bruto_word <- bruto %>%
unnest_tokens(word, txt)
#Pegar o bruto savedrecs do WoS formatado no Excel
bruto <- read.delim('lamano_text.txt', header = F,
colClasses = 'character')
bruto_word <- bruto %>%
unnest_tokens(word, txt)
#separar as palavras dos rotulo de dados
bruto <- data_frame(txt = savedrecs$V1) #usar rotulos do WoS
bruto_word <- bruto %>%
unnest_tokens(word, txt)
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t')
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t',
colClasses = 'character')
bruto_word <- bruto_word %>%
anti_join(stop_words)
bruto_word <- bruto %>%
unnest_tokens(word, txt)
bruto_word <- bruto_word %>%
anti_join(stop_words)
View(stop_words)
data = data("stop_words")
data = data(stop_words)
View(bruto_word)
data = data(stop_words)
data(stop_words)
View(stop_words)
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t',
colClasses = 'character')
data(stop_words)
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t',
colClasses = 'character', col.names = word)
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t',
colClasses = 'character', col.names = 'word')
stop_words$lexicon = 'SMART'
bruto_word <- bruto_word %>%
anti_join(stop_words)
#contar frequencia das palavras do rotulo de dados
freq_word <- bruto_word %>%
count(word, sort = TRUE)
#relação de bigramas do rotulo de dados removendo stop words e gerando frequencia
bruto_bigrams <- bruto %>%
unnest_tokens(bigram, txt, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
#relação de trigramas do rotulo de dados removendo stop words e gerando frequencia
bruto_trigrams <- bruto %>%
unnest_tokens(trigram, txt, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
#pairwise correlation das palavras do rotulo de dados removendo stop words
bruto_section_words <- bruto %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0) %>%
unnest_tokens(word, txt) %>%
filter(!word %in% stop_words$word)
# count words co-occuring within sections
bruto_word_pairs <- bruto_section_words %>%
pairwise_count(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
bruto_word_cors <- bruto_section_words %>%
group_by(word) %>%
filter(n() >= 8) %>% #filtrar para frequencia minima
pairwise_cor(word, section, sort = TRUE)
#grafico de frequencia bem bonito
bruto_word %>%
count(word, sort = TRUE) %>%
filter(n > 10) %>% #filtrar para a frequencia minima do gráfico
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#Gráfico de Bigram
bruto_bigram_graph <- bruto_bigrams %>%
filter(n > 3) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_bigram_graph
set.seed(2017)
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
freq_word
#remover stop words portugues baixado de
#https://gist.github.com/alopes/5358189
stop_words = read.delim('stopwords.txt', header = F, sep = '\t',
colClasses = 'character', col.names = 'word')
stop_words$lexicon = 'SMART'
bruto_word <- bruto_word %>%
anti_join(stop_words)
#contar frequencia das palavras do rotulo de dados
freq_word <- bruto_word %>%
count(word, sort = TRUE)
freq_word
#relação de bigramas do rotulo de dados removendo stop words e gerando frequencia
bruto_bigrams <- bruto %>%
unnest_tokens(bigram, txt, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
#relação de trigramas do rotulo de dados removendo stop words e gerando frequencia
bruto_trigrams <- bruto %>%
unnest_tokens(trigram, txt, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
#pairwise correlation das palavras do rotulo de dados removendo stop words
bruto_section_words <- bruto %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0) %>%
unnest_tokens(word, txt) %>%
filter(!word %in% stop_words$word)
# count words co-occuring within sections
bruto_word_pairs <- bruto_section_words %>%
pairwise_count(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
bruto_word_cors <- bruto_section_words %>%
group_by(word) %>%
filter(n() >= 8) %>% #filtrar para frequencia minima
pairwise_cor(word, section, sort = TRUE)
#grafico de frequencia bem bonito
bruto_word %>%
count(word, sort = TRUE) %>%
filter(n > 10) %>% #filtrar para a frequencia minima do gráfico
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#grafico de frequencia bem bonito
bruto_word %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>% #filtrar para a frequencia minima do gráfico
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#Gráfico de Bigram
bruto_bigram_graph <- bruto_bigrams %>%
filter(n > 3) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_bigram_graph
set.seed(2017)
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Bigram
bruto_bigram_graph <- bruto_bigrams %>%
filter(n > 2) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_bigram_graph
set.seed(2017)
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Trigram
bruto_trigram_graph <- bruto_trigrams %>%
filter(n >= 1) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_trigram_graph
set.seed(2017)
ggraph(bruto_trigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Trigram
bruto_trigram_graph <- bruto_trigrams %>%
filter(n > 1) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_trigram_graph
set.seed(2017)
ggraph(bruto_trigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Bigram com sombreamento e flecha
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
rm(a)
#Gráfico de Bigram
bruto_bigram_graph <- bruto_bigrams %>%
filter(n >= 2) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_bigram_graph
set.seed(2017)
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Trigram
bruto_trigram_graph <- bruto_trigrams %>%
filter(n >= 1) %>% #filtrar frequencia mínima
graph_from_data_frame()
bruto_trigram_graph
set.seed(2017)
ggraph(bruto_trigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#Gráfico de Bigram com sombreamento e flecha
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bruto_bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
rm(a)
#gráficos legais escolhendo palavras para correlacionar
bruto_word_cors %>%
filter(item1 %in% c("institutional", "strategy", "research", "education", "teaching")) %>% #escolher as palavras
group_by(item1) %>%
top_n(6) %>% #escolher o topN
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
#diagrama neural das palavras para correlacionar
set.seed(2016)
bruto_word_cors %>%
filter(correlation > .05) %>% #filtrar a correlacao minima
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
q()
library(rvest)
library(tm222)
